{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7dffd1-f2cd-45bf-a230-31055120c834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/13 09:15:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.36:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Distributed Shared Variables</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109d2b4a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Distributed Shared Variables\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7e1ed4-188e-4ee6-aaee-9ca5a51d09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EMP CSV data\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/input/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f149b6d1-fae2-4839-9b9d-9c96bc752c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable (Lookup)\n",
    "\n",
    "dept_names = {1 : 'Department 1', \n",
    "              2 : 'Department 2', \n",
    "              3 : 'Department 3', \n",
    "              4 : 'Department 4',\n",
    "              5 : 'Department 5', \n",
    "              6 : 'Department 6', \n",
    "              7 : 'Department 7', \n",
    "              8 : 'Department 8', \n",
    "              9 : 'Department 9', \n",
    "              10 : 'Department 10'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69cf3cd9-fc4a-440b-aaa8-4f355bdf251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast the variable\n",
    "\n",
    "dept_names_broadcast = spark.sparkContext.broadcast(dept_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eda44d6-c8be-457f-96ce-d29149966cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Department 1',\n",
       " 2: 'Department 2',\n",
       " 3: 'Department 3',\n",
       " 4: 'Department 4',\n",
       " 5: 'Department 5',\n",
       " 6: 'Department 6',\n",
       " 7: 'Department 7',\n",
       " 8: 'Department 8',\n",
       " 9: 'Department 9',\n",
       " 10: 'Department 10'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the value of the variable\n",
    "# broadcast variable is useful in case to supply the lookup variable \n",
    "# instead of doing actual shuffle\n",
    "\n",
    "dept_names_broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a3e6efe-ecf7-4317-9ed2-245d635124a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+----------+--------------------+-------------+--------+-------------+------------+\n",
      "|first_name|last_name|           job_title|       dob|               email|        phone|  salary|department_id|   dept_name|\n",
      "+----------+---------+--------------------+----------+--------------------+-------------+--------+-------------+------------+\n",
      "|   Richard| Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|(699)525-4827|512653.0|            8|Department 8|\n",
      "+----------+---------+--------------------+----------+--------------------+-------------+--------+-------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create UDF to return Department name\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf\n",
    "def dept_name_mapping(dept_id):\n",
    "    return dept_names_broadcast.value.get(dept_id)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "emp_final = emp.withColumn(\"dept_name\", dept_name_mapping(col(\"department_id\")))\n",
    "emp_final.show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9869098d-34c1-4dc8-a981-cc67319b8b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------+\n",
      "|department_id|CAST(sum(salary) AS BIGINT)|\n",
      "+-------------+---------------------------+\n",
      "|            6|                50294510721|\n",
      "+-------------+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate total salary of Department 6\n",
    "from pyspark.sql.functions import sum\n",
    "emp_dept_6_sal = emp.where(\"department_id == 6 \").groupBy(\"department_id\").agg(sum(\"salary\").cast(\"long\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b16e3b30-9dba-428a-a035-c2eb0f73de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulators\n",
    "\n",
    "emp_accumulators = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ff7a76f-99c9-4e4b-ba49-654d3fa09137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use foreach\n",
    "\n",
    "def calculate_dept_salary(department_id, salary):\n",
    "    if department_id ==6:\n",
    "        emp_accumulators.add(salary)\n",
    "\n",
    "emp.foreach(lambda row: calculate_dept_salary(row.department_id, row.salary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd812f20-2fb5-48ae-9765-7030b057c9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50294510721.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_accumulators.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f916c9b9-5b3a-45ac-b742-4c33eb19ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79994e64-aea2-4fe6-8f12-c972393a85e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
