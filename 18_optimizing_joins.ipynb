{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298341df-a541-4325-8b79-efc5659b24a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.36:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimizing Joins</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11ab0f4a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Joins\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 16)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24834ab6-01eb-4dcf-a719-a50081b64cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28925ab4-0c87-49bd-bfeb-7a5263e88209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join big and small tables - SortMerge vs BroadCast Join\n",
    "\n",
    "# Read EMP CSV data\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/input/employee_records.csv\")\n",
    "\n",
    "# Read DEPT CSV data\n",
    "\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"data/input/department_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d3bc7a5-d952-4ccb-9b67-c4bef0ac2810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join Datasets using SortMerge Join \n",
    "\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c38767c-1d8a-43ed-8093-d972985eae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [department_id#39], [department_id#48], LeftOuter\n",
      ":- *(1) Sort [department_id#39 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(department_id#39, 200), ENSURE_REQUIREMENTS, [plan_id=70]\n",
      ":     +- FileScan csv [first_name#32,last_name#33,job_title#34,dob#35,email#36,phone#37,salary#38,department_id#39] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/employee_records...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- *(3) Sort [department_id#48 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(department_id#48, 200), ENSURE_REQUIREMENTS, [plan_id=82]\n",
      "      +- *(2) Filter isnotnull(department_id#48)\n",
      "         +- FileScan csv [department_id#48,department_name#49,description#50,city#51,state#52,country#53] Batched: false, DataFilters: [isnotnull(department_id#48)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d553ec9a-ede0-4665-a6bb-077d915e9053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join Datasets using broadcast\n",
    "# Preferred only when one table is small and other is big\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = emp.join(broadcast(dept), on=emp.department_id==dept.department_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "267359ee-5e0b-4722-a41f-4b519b358537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [department_id#39], [department_id#48], LeftOuter, BuildRight, false\n",
      ":- FileScan csv [first_name#32,last_name#33,job_title#34,dob#35,email#36,phone#37,salary#38,department_id#39] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/employee_records...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=160]\n",
      "   +- *(1) Filter isnotnull(department_id#48)\n",
      "      +- FileScan csv [department_id#48,department_name#49,description#50,city#51,state#52,country#53] Batched: false, DataFilters: [isnotnull(department_id#48)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c2dbaa-41f3-41ed-b000-ebf34935e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Big and Big table - SortMerge without Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28d1da58-9cf5-4066-bdd6-7aa4b54a9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales data\n",
    "\n",
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"data/input/new_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d04006c-7bfc-4613-9fdb-c3b516bce0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"data/input/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32911f19-7092-469f-b71d-56a8f8714768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join Datasets using SortMerge Join \n",
    "\n",
    "df_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db5b0f25-c15f-4f04-b5aa-6165a7c74500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [city_id#235], [city_id#242], LeftOuter\n",
      ":- *(1) Sort [city_id#235 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(city_id#235, 200), ENSURE_REQUIREMENTS, [plan_id=279]\n",
      ":     +- FileScan csv [transacted_at#230,trx_id#231,retailer_id#232,description#233,amount#234,city_id#235] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/new_sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit...\n",
      "+- *(3) Sort [city_id#242 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(city_id#242, 200), ENSURE_REQUIREMENTS, [plan_id=291]\n",
      "      +- *(2) Filter isnotnull(city_id#242)\n",
      "         +- FileScan csv [city_id#242,city#243,state#244,state_abv#245,country#246] Batched: false, DataFilters: [isnotnull(city_id#242)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/cities.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ec88d84-1ec7-4945-a9f5-c3584719b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/13 18:39:51 WARN MemoryStore: Not enough space to cache broadcast_27 in memory! (computed 432.0 MiB so far)\n",
      "25/01/13 18:39:51 WARN BlockManager: Persisting block broadcast_27 to disk instead.\n",
      "25/01/13 18:39:53 WARN MemoryStore: Not enough space to cache broadcast_27 in memory! (computed 432.0 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join Datasets using SortMerge Join \n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = sales.join(broadcast(city), on=sales.city_id==city.city_id, how=\"left_outer\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "\n",
    "# Expected to fail due to Out of Memeory Issue \n",
    "# This leads to either spill the data on Disk or increase input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "326bb0a9-8d00-4a25-9b50-941967a6f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [city_id#235], [city_id#242], LeftOuter, BuildRight, false\n",
      ":- FileScan csv [transacted_at#230,trx_id#231,retailer_id#232,description#233,amount#234,city_id#235] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/new_sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit...\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=369]\n",
      "   +- *(1) Filter isnotnull(city_id#242)\n",
      "      +- FileScan csv [city_id#242,city#243,state#244,state_abv#245,country#246] Batched: false, DataFilters: [isnotnull(city_id#242)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/cities.csv], PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c547bf-38b7-4f3b-a6d1-bbdd745bf094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Spark murmur hasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0647c124-9ba5-4bf6-8938-d6978197d97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write Sales and City data in Buckets\n",
    "# import os\n",
    "# os.makedirs(\"/data/input/datasets/\", exist_ok=True)\n",
    "\n",
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"/data/input/datasets/sales_bucket.csv\").saveAsTable(\"sales_bucket\")\n",
    "\n",
    "# Total_bukcet_created = 8 (parttions) * 4 (buckets in each partitions) = 32 buckets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "850a1291-adb9-413c-b94d-2499c56675fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write City data in Buckets\n",
    "\n",
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).option(\"path\", \"data/input/datasets/city_bucket.csv\").saveAsTable(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "215434ab-17c8-4ae7-a627-9d1bc38a7755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default| city_bucket|      false|\n",
      "|  default|sales_bukcet|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if spark.catalog.tableExists(\"city_bucket\"):\n",
    "#     print(\"Table sales_bukcet exists.\")\n",
    "# else:\n",
    "#     print(\"Table sales_bukcet does not exist.\")\n",
    "\n",
    "# Check tables\n",
    "\n",
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "583f9ff6-e96e-45fd-afb4-10051d563216",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_bucket = spark.read.table(\"sales_bukcet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1c12e1d-eb85-49d4-a758-e197b53908ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_bucket = spark.read.table(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "483d0bfc-5fd0-41e9-b642-4f0206ef56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_bucket = sales_bucket.join(city_bucket, on= sales_bucket.city_id==city_bucket.city_id, how = \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecfae120-a62e-41bb-a263-aafd71c066d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined_bucket.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e77584e-e580-43a2-adcb-d451f8b73c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) SortMergeJoin [city_id#490], [city_id#497], LeftOuter\n",
      ":- *(1) Sort [city_id#490 ASC NULLS FIRST], false, 0\n",
      ":  +- FileScan csv spark_catalog.default.sales_bukcet[transacted_at#485,trx_id#486,retailer_id#487,description#488,amount#489,city_id#490] Batched: false, Bucketed: true, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/spark-warehouse/data/input/d..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit..., SelectedBucketsCount: 4 out of 4\n",
      "+- *(2) Sort [city_id#497 ASC NULLS FIRST], false, 0\n",
      "   +- *(2) Filter isnotnull(city_id#497)\n",
      "      +- FileScan csv spark_catalog.default.city_bucket[city_id#497,city#498,state#499,state_abv#500,country#501] Batched: false, Bucketed: true, DataFilters: [isnotnull(city_id#497)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/spark-warehouse/data/input/d..., PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_bucket.explain()\n",
    "# noShuffle involved here\n",
    "# 4 tasks were ran parallel as we've 4 buckets here\n",
    "# For ex. Task 0, reads total 2026174 which includes records from both sales and city tables from \n",
    "# 0th buckets \n",
    "\n",
    "# Key Note: \n",
    "# Data spilled on Disk memory and there is scope for optimsation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2572c342-dc03-4588-aae8-1ee73f32749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Selection Strategies in PySpark\n",
    "# SORT_MERGE, SHUFFLE_JOIN and BROADCAST_HASH_JOIN\n",
    "\n",
    "# If join column is not same as bucketing column and same bucket size - data will shuffle\n",
    "# If join column same as bucketing column and one table bucket - data will shuffle on smaller table\n",
    "# If join column same as bucketing column and different bucket size - data will shuffle on smaller table\n",
    "# If join column same as bucketing column and same bucket size - No shuffle (Fast op)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
