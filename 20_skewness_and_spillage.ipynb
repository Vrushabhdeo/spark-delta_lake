{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "025d43db-836d-4682-a60c-5c5ba710febb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.33:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimizing Skewness and Spillage</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x169d09b80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Skewness and Spillage\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.cores.max\", 8)\n",
    "    .config(\"spark.executor.cores\", 4)\n",
    "    .config(\"spark.executor.memory\", \"512M\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f62f96c-8246-4390-a865-8c5d21e410f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd66de82-ddaa-4372-83bc-0ac712aa118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Employee data\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/input/employee_records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a827a137-c039-44ed-95c6-e8eb1d44e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DEPT CSV data\n",
    "_dept_schema = \"department_id int, department_name string, description string, city string, state string, country string\"\n",
    "\n",
    "dept = spark.read.format(\"csv\").schema(_dept_schema).option(\"header\", True).load(\"data/input/department_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42d7a8f3-34f9-4e06-8715-022ed1bcadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Datasets\n",
    "\n",
    "df_joined = emp.join(dept, on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9e70cbb-0a17-45d1-802c-23866abd7702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d9d0962-d9b2-4179-b6de-b121497e5c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) SortMergeJoin [department_id#511], [department_id#520], LeftOuter\n",
      ":- *(1) Sort [department_id#511 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(department_id#511, 200), ENSURE_REQUIREMENTS, [plan_id=607]\n",
      ":     +- FileScan csv [first_name#504,last_name#505,job_title#506,dob#507,email#508,phone#509,salary#510,department_id#511] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/employee_records...., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<first_name:string,last_name:string,job_title:string,dob:string,email:string,phone:string,s...\n",
      "+- *(3) Sort [department_id#520 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(department_id#520, 200), ENSURE_REQUIREMENTS, [plan_id=619]\n",
      "      +- *(2) Filter isnotnull(department_id#520)\n",
      "         +- FileScan csv [department_id#520,department_name#521,description#522,city#523,state#524,country#525] Batched: false, DataFilters: [isnotnull(department_id#520)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/vrushabh.deokar/pysparkBasics/data/input/department_data.csv], PartitionFilters: [], PushedFilters: [IsNotNull(department_id)], ReadSchema: struct<department_id:int,department_name:string,description:string,city:string,state:string,count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4212c1e-5617-4ee4-b843-eb1a10f463ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|partition_num| count|\n",
      "+-------------+------+\n",
      "|          103|100417|\n",
      "|          122| 99780|\n",
      "|           43| 99451|\n",
      "|          107| 99805|\n",
      "|           49| 99706|\n",
      "|           51|100248|\n",
      "|          102|100214|\n",
      "|           66|100210|\n",
      "|          174|100155|\n",
      "|           89|100014|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "\n",
    "part_df = df_joined.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "part_df.show()\n",
    "\n",
    "# Obersvation - Task Index (102, 103, 174) saw spillage on disk\n",
    "# Task Index = partition_num\n",
    "# Also from Default 200 Shuffled partition, 190 are idle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f69c9b-be56-4804-b0c5-d60314ab68c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|department_id| count|\n",
      "+-------------+------+\n",
      "|            1| 99451|\n",
      "|            6| 99706|\n",
      "|            3|100248|\n",
      "|            5|100210|\n",
      "|            9|100014|\n",
      "|            4|100214|\n",
      "|            8|100417|\n",
      "|            7| 99805|\n",
      "|           10| 99780|\n",
      "|            2|100155|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Employee data based on department_id\n",
    "\n",
    "from pyspark.sql.functions import col, count, lit, desc\n",
    "\n",
    "emp.groupBy(\"department_id\").agg(count(lit(1)).alias(\"count\")).show()\n",
    "\n",
    "# Repartition will not work as department_id like 2, 3, 5 etc will again come to same partition \n",
    "# and hence Salting Technique comes into picture\n",
    "# Approach - Add Salting to dept_id, do Shuffle and then remove Salting from dept_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11b4b10c-49d2-4123-afd0-7a895d70cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set shuffle partitions to a lesser number - 8\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6df6f3e9-1b92-4b00-8a77-a699dd480246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let prepare the salt\n",
    "\n",
    "import random\n",
    "from pyspark.sql.functions import udf \n",
    "\n",
    "@udf \n",
    "def rand_udf():\n",
    "    return random.randint(0,8)\n",
    "\n",
    "salt_df = spark.range(0,8)\n",
    "salt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab69ddfc-9193-43de-ad04-97ab62f28253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------------------------------+----------+----------------------------+---------------------+--------+-------------+--------------+\n",
      "|first_name|last_name |job_title                         |dob       |email                       |phone                |salary  |department_id|salted_dept_id|\n",
      "+----------+----------+----------------------------------+----------+----------------------------+---------------------+--------+-------------+--------------+\n",
      "|Richard   |Morrison  |Public relations account executive|1973-05-05|melissagarcia@example.org   |(699)525-4827        |512653.0|8            |8_3           |\n",
      "|Bobby     |Mccarthy  |Barrister's clerk                 |1974-04-25|llara@example.net           |(750)846-1602x7458   |999836.0|7            |7_4           |\n",
      "|Dennis    |Norman    |Land/geomatics surveyor           |1990-06-24|jturner@example.net         |873.820.0518x825     |131900.0|10           |10_6          |\n",
      "|John      |Monroe    |Retail buyer                      |1968-06-16|erik33@example.net          |820-813-0557x624     |485506.0|1            |1_5           |\n",
      "|Michelle  |Elliott   |Air cabin crew                    |1975-03-31|tiffanyjohnston@example.net |(705)900-5337        |604738.0|8            |8_5           |\n",
      "|Ashley    |Montoya   |Cartographer                      |1976-01-16|patrickalexandra@example.org|211.440.5466         |483339.0|6            |6_6           |\n",
      "|Nathaniel |Smith     |Quality manager                   |1985-06-28|lori44@example.net          |936-403-3179         |419644.0|7            |7_8           |\n",
      "|Faith     |Cummings  |Industrial/product designer       |1978-07-01|ygordon@example.org         |(889)246-5588        |205939.0|7            |7_5           |\n",
      "|Margaret  |Sutton    |Administrator, education          |1975-08-16|diana44@example.net         |001-647-530-5036x7523|671167.0|8            |8_1           |\n",
      "|Mary      |Sutton    |Freight forwarder                 |1979-12-28|ryan36@example.com          |422.562.7254x3159    |993829.0|7            |7_0           |\n",
      "|Jake      |King      |Lexicographer                     |1994-07-11|monica93@example.org        |+1-535-652-9715x66854|702101.0|4            |4_7           |\n",
      "|Heather   |Haley     |Music tutor                       |1981-06-01|stephanie65@example.net     |(652)815-7973x298    |570960.0|6            |6_0           |\n",
      "|Thomas    |Thomas    |Chartered management accountant   |2001-07-17|pwilliams@example.com       |001-245-848-0028x5105|339441.0|6            |6_8           |\n",
      "|Leonard   |Carlson   |Art therapist                     |1990-10-18|gabrielmurray@example.com   |9247590563           |469728.0|8            |8_3           |\n",
      "|Mark      |Wood      |Market researcher                 |1963-10-13|nicholas76@example.com      |311.439.1606x3342    |582291.0|4            |4_1           |\n",
      "|Tracey    |Washington|Travel agency manager             |1986-05-07|mark07@example.com          |001-912-206-6456     |146456.0|4            |4_7           |\n",
      "|Rachael   |Rodriguez |Media buyer                       |1966-12-02|griffinmary@example.org     |+1-791-344-7586x548  |544732.0|1            |1_0           |\n",
      "|Tara      |Liu       |Financial adviser                 |1998-10-12|alexandraobrien@example.org |216.696.6061         |399503.0|3            |3_4           |\n",
      "|Ana       |Joseph    |Retail manager                    |1995-01-10|rmorse@example.org          |(726)363-7526x9965   |761988.0|10           |10_5          |\n",
      "|Richard   |Hall      |Engineer, civil (contracting)     |1967-03-02|brandoncardenas@example.com |(964)451-9007x22496  |660659.0|4            |4_0           |\n",
      "+----------+----------+----------------------------------+----------+----------------------------+---------------------+--------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted Employee\n",
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "salted_emp = emp.withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), rand_udf()))\n",
    "salted_emp.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b55f833-4220-4e2f-a2a6-168803b41832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+--------------------+------------+-----+-------------------+---+--------------+\n",
      "|department_id|department_name|         description|        city|state|            country| id|salted_dept_id|\n",
      "+-------------+---------------+--------------------+------------+-----+-------------------+---+--------------+\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  0|           1_0|\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  1|           1_1|\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  2|           1_2|\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  3|           1_3|\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  4|           1_4|\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  5|           1_5|\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  6|           1_6|\n",
      "|            1|    Bryan-James|Optimized disinte...|Melissaburgh|   FM|Trinidad and Tobago|  7|           1_7|\n",
      "+-------------+---------------+--------------------+------------+-----+-------------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Salted Department\n",
    "salted_dept = dept.join(salt_df, how=\"cross\").withColumn(\"salted_dept_id\", concat(\"department_id\", lit(\"_\"), \"id\" ))\n",
    "salted_dept.where(\"department_id == 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00f4643-7fe9-45b3-a0dc-96aea8351b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make the salted join now\n",
    "\n",
    "df_salted_joined = salted_emp.join(salted_dept, on=emp.department_id==dept.department_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef52d747-9e23-4510-b3b3-8d361d4a6ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_salted_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84bb77ee-cabd-4ac3-9c7d-279b2f74e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|partition_num| count|\n",
      "+-------------+------+\n",
      "|            6|130312|\n",
      "|            5|130403|\n",
      "|            1|130406|\n",
      "|            3|130393|\n",
      "|            7| 87281|\n",
      "|            2|130400|\n",
      "|            4|130384|\n",
      "|            0|130421|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the partition details to understand distribution\n",
    "\n",
    "from pyspark.sql.functions import spark_partition_id, count, lit\n",
    "\n",
    "part_df = salted_emp.withColumn(\"partition_num\", spark_partition_id()).groupBy(\"partition_num\").agg(count(lit(1)).alias(\"count\"))\n",
    "part_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f698624-7e2b-4ebb-bf99-e07add70eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2115576c-c880-4860-8f46-49a98b1fc4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Skewness? - Basically unbalanced data not distributed equally \n",
    "# Type of Spillage - \n",
    "# Spill Memory - deserialised, Spilled on Memory \n",
    "# Spill Disk - serialised, Spilled on Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8438845-4780-4167-b9f2-ec709cf60b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
