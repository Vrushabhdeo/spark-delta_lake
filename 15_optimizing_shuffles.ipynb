{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0c8033-86a5-46b3-9668-867a39d46158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/12 18:05:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.36:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Optimizing Shuffles</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x107c074a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Optimizing Shuffles\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43895b72-14ac-4652-a963-7d915604cfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Spark defaultParallelism\n",
    "\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a405fe1-e203-456c-8dc7-99ad3bd42c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable AQE and Broadcast join\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70737419-cfb4-40bf-bc28-b3ac4fbdb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EMP CSV file with 10M records\n",
    "\n",
    "_schema = \"first_name string, last_name string, job_title string, dob string, email string, phone string, salary double, department_id int\"\n",
    "\n",
    "emp = spark.read.format(\"csv\").schema(_schema).option(\"header\", True).load(\"data/input/employee_records.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99806bd1-ca16-492e-9c33-7cea8fb08934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out avg salary as per dept\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "emp_avg = emp.groupBy(\"department_id\").agg(avg(\"salary\")).alias(\"avg_sal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0048272-10e5-4c14-a59d-741028c95e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write data for performance Benchmarking\n",
    "\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01edbc6f-e010-4d28-adfa-0ee334ff7caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Spark Shuffle Partition setting\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76262209-7677-49f3-b6c5-fac0520dc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
    "\n",
    "# adjusting too many partitions results tasks busy with network and disk io\n",
    "# adjusting to low partitions may results tasks OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12a5a29e-eb0c-42f5-bab0-d38b768cf75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+----------+--------------------+-------------+--------+-------------+------------+\n",
      "|first_name|last_name|           job_title|       dob|               email|        phone|  salary|department_id|partition_id|\n",
      "+----------+---------+--------------------+----------+--------------------+-------------+--------+-------------+------------+\n",
      "|   Richard| Morrison|Public relations ...|1973-05-05|melissagarcia@exa...|(699)525-4827|512653.0|            8|           0|\n",
      "+----------+---------+--------------------+----------+--------------------+-------------+--------+-------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "emp.withColumn(\"partition_id\", spark_partition_id()).where(\"partition_id = 0\").show(1)\n",
    "\n",
    "# each tasks(10 tasks for 10 department_id) run on each partitons(8) \n",
    "# resulting 10 records read for each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6b9c981-2f32-4d5c-b794-033b42e30f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the partitioned data\n",
    "\n",
    "emp_part = spark.read.format(\"csv\").schema(_schema).load(\"data/output/11/2/emp.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00280f38-57a8-4221-9e9e-1016aecd78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "emp_avg = emp_part.groupBy(\"department_id\").agg(avg(\"salary\")).alias(\"avg_sal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86f4e613-1fd5-4c1d-b6bf-f8c7d3b3fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data for performance Benchmarking\n",
    "# partitoned read reduced the shuffle write from 5.6 kb to 953 B \n",
    "\n",
    "emp_avg.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5d2d726-b80a-4675-b407-fb930c6217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Shuffling practices\n",
    "# 1. repartition data properly\n",
    "# 2. filter out the data in early stages \n",
    "# 3. keep as much as less data for shuffling\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98765c61-c8b6-4cb6-a5ca-81285912a4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
